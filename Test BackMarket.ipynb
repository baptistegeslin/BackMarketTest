{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BackMarket - Baptiste Geslin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pipeline Assessment.\n",
    "\n",
    "You can develop & refactor your code (using your versioning tool) following this pipeline:\n",
    "\n",
    "1 - Download and read the file: product_catalog.csv locally <br>\n",
    "2 - Transform the file from CSV to Parquet format locally <br>\n",
    "3 - Separate the valid rows from the invalid ones into two separate files: the business wants only the product with an image but wants to archive the invalids rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, well done!\n",
    "Now Back Market is growing so fast, what you would do to tackle the massive new CSV files?\n",
    "Describe the next steps for your code to scale it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'ARROW_PRE_0_15_IPC_FORMAT' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=0.15 and pyspark<3.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import databricks.koalas as ks\n",
    "import unittest\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PANDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a python library dedicated to datascientist. It provides high-performance, easy-to-use data structures and data analysis tools that let the user explore, analyse and manipulate datasets.<br>\n",
    "In our case, product_catalogs.csv's size is about 1Mo so the use of Pandas is appropriate. <br>\n",
    "For larger dataset, it may be necessary to use a large-scale data processing tool like Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineWithPandas(path):\n",
    "    \"\"\"\n",
    "    This function imports product_catalog.csv file located in the given path.\n",
    "    It splits the data into 2 parquet files :\n",
    "        - valid rows with an image specified\n",
    "        - invalid rows without image supplied\n",
    "    This function uses the pandas library.\n",
    "    \"\"\"\n",
    "       \n",
    "    # import product_catalog.csv\n",
    "    product_catalog = pd.read_csv(path,sep=\",\",header=0,  encoding=\"utf-8\")\n",
    "    \n",
    "    # separate the data into two dataframes : invalid_rows and valid_rows\n",
    "    invalid_rows = product_catalog[product_catalog['image'].isnull()]\n",
    "    valid_rows = product_catalog[product_catalog['image'].notnull()] \n",
    "    \n",
    "    # Il est possible d'ordonner les entrées selon les champs les plus fréquemment utilisés lors des requêtes\n",
    "    # afin d'améliorer leurs performances.\n",
    "    # Ici je propose de trier la colonne \"category_id\" afin de pouvoir requêter de manière efficace les produits d'une même catégorie.\n",
    "    valid_rows = valid_rows.sort_values(by=['category_id'])\n",
    "    invalid_rows = invalid_rows.sort_values(by=['category_id'])\n",
    "    \n",
    "    # create parquet files locally\n",
    "    invalid_rows.to_parquet('invalid_rows_pandas.parquet', compression='snappy')\n",
    "    valid_rows.to_parquet('valid_rows_pandas.parquet', compression='snappy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark is a unified analytics engine for large-scale data processing. \n",
    "It's a great way to tackle massive new csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SparkContext and SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineWithSpark(path): \n",
    "    \"\"\"\n",
    "    This function imports product_catalog.csv file located in the given path.\n",
    "    It splits the file into 2 parquet files :\n",
    "        - valid rows with an image specified\n",
    "        - invalid rows without image supplied\n",
    "    This function uses the spark engine.\n",
    "    \"\"\"\n",
    "    \n",
    "    # import product_catalog.csv\n",
    "    product_catalog = ss.read.load(path, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "    \n",
    "    # separate the data into two dataframes : invalid_rows and valid_rows\n",
    "    invalid_rows = product_catalog.where(product_catalog[\"image\"].isNull())\n",
    "    valid_rows = product_catalog.where(product_catalog[\"image\"].isNotNull())\n",
    "    \n",
    "    # create parquet files locally\n",
    "    invalid_rows.write.mode('overwrite').parquet('invalid_rows_pyspark.parquet')\n",
    "    valid_rows.write.mode('overwrite').parquet('valid_rows_pyspark.parquet') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KOALAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koalas (Databricks open source project) implements Pandas API on top of Apache Spark. <br>\n",
    "It brings together Spark's distributed environment and high performance along with Pandas easy to use API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineWithKoalas(path):\n",
    "    \"\"\"\n",
    "    This function imports product_catalog.csv file located in the given path.\n",
    "    It splits the file into 2 parquet files :\n",
    "        - valid rows with an image specified\n",
    "        - invalid rows without image supplied\n",
    "    This function uses the koalas library.\n",
    "    \"\"\"\n",
    "    \n",
    "    # import product_catalog.csv   \n",
    "    product_catalog = ss.read.load(path, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\").to_koalas()\n",
    "    \n",
    "    # separate the data into two dataframes : invalid_rows and valid_rows\n",
    "    invalid_rows = product_catalog[product_catalog[\"image\"].isna()]\n",
    "    valid_rows = product_catalog.dropna(subset=['image'])\n",
    "    \n",
    "    # create parquet files locally\n",
    "    invalid_rows.to_parquet('invalid_rows_koalas.parquet', mode = 'overwrite')\n",
    "    valid_rows.to_parquet('valid_rows_koalas.parquet', mode = 'overwrite') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNITEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unit tests for Pipeline functions : \n",
    "    - PipelineWithSpark\n",
    "    - PipelineWithKoalas\n",
    "    - PipelineWithPandas\n",
    "\n",
    "\"\"\"\n",
    "class PipelineFunctionTest(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(self):\n",
    "        # Define path\n",
    "        self.path = \"product_catalog.csv\"\n",
    "    \n",
    "        \n",
    "    def testPipelineWithSpark(self):\n",
    "        # Run Spark pipeline\n",
    "        PipelineWithSpark(self.path)\n",
    "        \n",
    "        # Read parquet files with pyarrow\n",
    "        valid_spark = pq.read_table('valid_rows_pyspark.parquet').to_pandas()\n",
    "        invalid_spark = pq.read_table('invalid_rows_pyspark.parquet').to_pandas()\n",
    "        \n",
    "        # Perform tests\n",
    "        self.assertFalse(valid_spark[\"image\"].isnull().any())\n",
    "        self.assertTrue(invalid_spark[\"image\"].isnull().all())\n",
    "        \n",
    "        \n",
    "    def testPipelineWithPandas(self):\n",
    "        # Run Pandas pipeline\n",
    "        PipelineWithPandas(self.path)\n",
    "        \n",
    "        # Read parquet files with pyarrow\n",
    "        valid_pandas = pq.read_table('valid_rows_pandas.parquet').to_pandas()\n",
    "        invalid_pandas = pq.read_table('invalid_rows_pandas.parquet').to_pandas()\n",
    "        \n",
    "        # Perform tests\n",
    "        self.assertFalse(valid_pandas[\"image\"].isnull().any())\n",
    "        self.assertTrue(invalid_pandas[\"image\"].isnull().all())\n",
    "        \n",
    "        \n",
    "    def testPipelineWithKoalas(self):\n",
    "        # Run Koalas pipeline\n",
    "        PipelineWithKoalas(self.path)\n",
    "        \n",
    "        # Read parquet files with pyarrow\n",
    "        valid_koalas = pq.read_table('valid_rows_koalas.parquet').to_pandas()\n",
    "        invalid_koalas = pq.read_table('invalid_rows_koalas.parquet').to_pandas()\n",
    "        \n",
    "        # Perform tests\n",
    "        self.assertFalse(valid_koalas[\"image\"].isnull().any())\n",
    "        self.assertTrue(invalid_koalas[\"image\"].isnull().all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testPipelineWithKoalas (__main__.PipelineFunctionTest) ... ok\n",
      "testPipelineWithPandas (__main__.PipelineFunctionTest) ... ok\n",
      "testPipelineWithSpark (__main__.PipelineFunctionTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 18.973s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the tests\n",
    "tests = unittest.makeSuite(PipelineFunctionTest)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
