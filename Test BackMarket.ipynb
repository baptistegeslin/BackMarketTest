{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BackMarket - Baptiste Geslin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pipeline Assessment.\n",
    "\n",
    "You can develop & refactor your code (using your versioning tool) following this pipeline:\n",
    "\n",
    "1 - Download and read the file: product_catalog.csv locally <br>\n",
    "2 - Transform the file from CSV to Parquet format locally <br>\n",
    "3 - Separate the valid rows from the invalid ones into two separate files: the business wants only the product with an image but wants to archive the invalids rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, well done!\n",
    "Now Back Market is growing so fast, what you would do to tackle the massive new CSV files?\n",
    "Describe the next steps for your code to scale it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'ARROW_PRE_0_15_IPC_FORMAT' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=0.15 and pyspark<3.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import databricks.koalas as ks\n",
    "import unittest\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineWithPandas(path):\n",
    "    # import product_catalog.csv\n",
    "    product_catalog = pd.read_csv(path,sep=\",\",header=0,  encoding=\"utf-8\")\n",
    "    \n",
    "    # separate the data into two dataframes : invalid_rows and valid_rows\n",
    "    invalid_rows = product_catalog[product_catalog['image'].isnull()]\n",
    "    valid_rows = product_catalog[product_catalog['image'].notnull()] \n",
    "    \n",
    "    # Il est possible d'ordonner les entrées selon les champs les plus fréquemment utilisés lors des requêtes\n",
    "    # afin d'améliorer leurs performances.\n",
    "    # Ici je propose de trier la colonne \"category_id\" afin de pouvoir requêter de manière efficace les produits d'une même catégorie.\n",
    "    valid_rows = valid_rows.sort_values(by=['category_id'])\n",
    "    invalid_rows = invalid_rows.sort_values(by=['category_id'])\n",
    "    \n",
    "    # create parquet files locally\n",
    "    invalid_rows.to_parquet('invalid_rows_pandas.parquet', compression='snappy')\n",
    "    valid_rows.to_parquet('valid_rows_pandas.parquet', compression='snappy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PANDAS est plus un outil dédié aux datascientist pour une exploration / analyse rapide / manipulation des données.<br>\n",
    "Pour un fichier csv d'environ 1Mo, PANDAS est suffisant. <br>\n",
    "Pour des volumes de données plus importants, il peut être nécessaire d'utiliser un outil de calcul distribué comme Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SparkContext and SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "ss = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineWithSpark(path):     \n",
    "    # import product_catalog.csv\n",
    "    product_catalog = ss.read.load(path, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "    \n",
    "    # separate the data into two dataframes : invalid_rows and valid_rows\n",
    "    invalid_rows = product_catalog.where(product_catalog[\"image\"].isNull())\n",
    "    valid_rows = product_catalog.where(product_catalog[\"image\"].isNotNull())\n",
    "    \n",
    "    # create parquet files locally\n",
    "    invalid_rows.write.mode('overwrite').parquet('invalid_rows_pyspark.parquet')\n",
    "    valid_rows.write.mode('overwrite').parquet('valid_rows_pyspark.parquet') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KOALAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineWithKoalas(path):        \n",
    "    # import product_catalog.csv   \n",
    "    product_catalog = ss.read.load(path, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\").to_koalas()\n",
    "    \n",
    "    # separate the data into two dataframes : invalid_rows and valid_rows\n",
    "    invalid_rows = product_catalog[product_catalog[\"image\"].isna()]\n",
    "    valid_rows = product_catalog.dropna(subset=['image'])\n",
    "    \n",
    "    # create parquet files locally\n",
    "    invalid_rows.to_parquet('invalid_rows_koalas.parquet', mode = 'overwrite')\n",
    "    valid_rows.to_parquet('valid_rows_koalas.parquet', mode = 'overwrite') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNITEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unit tests for Pipeline functions : PipelineWithSpark, PipelineWithKoalas and PipelineWithPandas\n",
    "\n",
    "\"\"\"\n",
    "class PipelineFunctionTest(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(self):\n",
    "        # Define path\n",
    "        self.path = \"product_catalog.csv\"\n",
    "    \n",
    "        \n",
    "    def testPipelineWithSpark(self):\n",
    "        PipelineWithSpark(self.path)\n",
    "        valid_spark = pq.read_table('valid_rows_pyspark.parquet').to_pandas()\n",
    "        invalid_spark = pq.read_table('invalid_rows_pyspark.parquet').to_pandas()\n",
    "        \n",
    "        self.assertFalse(valid_spark[\"image\"].isnull().any())\n",
    "        self.assertTrue(invalid_spark[\"image\"].isnull().any())\n",
    "        \n",
    "        \n",
    "    def testPipelineWithPandas(self):\n",
    "        PipelineWithPandas(self.path)\n",
    "        valid_pandas = pq.read_table('valid_rows_pandas.parquet').to_pandas()\n",
    "        invalid_pandas = pq.read_table('invalid_rows_pandas.parquet').to_pandas()\n",
    "        \n",
    "        self.assertFalse(valid_pandas[\"image\"].isnull().any())\n",
    "        self.assertTrue(invalid_pandas[\"image\"].isnull().any())\n",
    "        \n",
    "        \n",
    "    def testPipelineWithKoalas(self):\n",
    "        PipelineWithKoalas(self.path)\n",
    "        valid_koalas = pq.read_table('valid_rows_koalas.parquet').to_pandas()\n",
    "        invalid_koalas = pq.read_table('invalid_rows_koalas.parquet').to_pandas()\n",
    "        \n",
    "        self.assertFalse(valid_koalas[\"image\"].isnull().any())\n",
    "        self.assertTrue(invalid_koalas[\"image\"].isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testPipelineWithKoalas (__main__.PipelineFunctionTest) ... ok\n",
      "testPipelineWithPandas (__main__.PipelineFunctionTest) ... ok\n",
      "testPipelineWithSpark (__main__.PipelineFunctionTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 1.529s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute tests\n",
    "tests = unittest.makeSuite(PipelineFunctionTest)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
